# ğŸ›ï¸ Legal Advisory Chatbot

A **Legal Advisory Chatbot** designed to help EU businesses comply with **DORA (Digital Operational Resilience Act)** regulations using advanced Retrieval-Augmented Generation (RAG) techniques.

## ğŸ¯ Project Overview

This project provides an intelligent legal advisory system that assists EU businesses in understanding and complying with DORA regulations. The chatbot delivers precise, context-aware legal guidance by retrieving relevant regulatory information and generating professional advisory responses.

### Key Features
- ğŸ” **Intelligent Document Retrieval**: Advanced RAG pipeline with semantic search and reranking
- âš–ï¸ **Legal Expert Responses**: Professional legal advisor tone and terminology  
- ğŸ¯ **DORA Regulation Focus**: Specialized knowledge base for EU digital operational resilience
- ğŸš€ **Flexible LLM Options**: Support for both local (Ollama) and cloud-based (Gemini) models
- ğŸ“Š **Comprehensive Analytics**: Detailed retrieval analysis and reranking insights

## ğŸ—ï¸ Technical Architecture

### RAG Pipeline
```
ğŸ“„ DORA PDF â†’ ğŸ”ª Chunking â†’ ğŸ§® Embeddings â†’ ğŸ—„ï¸ FAISS Vector DB
                                                        â†“
ğŸ’¬ User Query â†’ ğŸ” Retrieval â†’ ğŸ¯ Reranking â†’ ğŸ¤– LLM Generation â†’ ğŸ“ Response
```

### Tech Stack
- **ğŸ“„ Document Processing**: PDF parsing and intelligent text chunking
- **ğŸ§® Embeddings**: `BAAI/bge-large-en-v1.5` (1024 dimensions)
- **ğŸ—„ï¸ Vector Database**: FAISS for efficient similarity search
- **ğŸ” Retrieval**: Semantic search with configurable top-k
- **ğŸ¯ Reranking**: `BAAI/bge-reranker-base` cross-encoder for relevance refinement
- **ğŸ¤– LLM Generation**: 
  - **Local**: Ollama (llama3.2-3B-Instruct)
  - **Cloud**: Google Gemini (2.0-flash, 1.5-flash, 1.5-pro)
- **ğŸ–¥ï¸ User Interface**: Streamlit for interactive PoC demonstration

## ğŸš€ Quick Start

### Prerequisites
```bash
# Python 3.8+
pip install -r requirements.txt

# For local LLM (optional)
# Install Ollama: https://ollama.ai/
ollama pull llama3.2

# For Gemini LLM
# in the .env file:
GOOGLE_API_KEY = <YOUR_GOOGLE_API_KEY>
```

### Environment Setup
1. Create a `.env` file:
```env
# Required for Gemini LLM
GOOGLE_API_KEY=your_gemini_api_key_here

# Optional Ollama settings
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2
```

2. Ensure DORA document is available:
```
ğŸ“ RAG_model/
â”œâ”€â”€ DORA.pdf                    # Source regulation document
â”œâ”€â”€ dora_chunks_simple.json     # Pre-processed chunks
â””â”€â”€ ...
```

### Running the Application
```bash
# Navigate to project directory
cd RAG_model

# Launch Streamlit interface
streamlit run frontend/app.py
```

Visit `http://localhost:8501` to access the chatbot interface.

## ğŸ“Š System Configuration

### Model Selection
- **ğŸ  Local Processing**: Choose Ollama for privacy and offline operation
- **â˜ï¸ Cloud Processing**: Choose Gemini for faster responses and advanced capabilities

### Retrieval Settings
- **Initial Retrieval**: 15-50 documents from vector database
- **Reranking**: Cross-encoder refinement for top-k selection
- **Final Output**: 5-15 most relevant chunks for context

### Performance Optimization
- **Smart Caching**: Embeddings and models cached for faster startup
- **Batch Processing**: Efficient document chunking and embedding generation
- **GPU Support**: Optional GPU acceleration for embedding models

## ğŸ”§ Project Structure

```
RAG_model/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ embeddings.py          # Embedding model and FAISS management
â”‚   â”œâ”€â”€ reranker.py            # Cross-encoder reranking logic
â”‚   â”œâ”€â”€ main.py                # RAG orchestration and LLM chains
â”‚   â”œâ”€â”€ utils.py               # Utility functions and document processing
â”‚   â””â”€â”€ generator_prompt.txt   # Legal advisor system prompt
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ app.py                 # Streamlit user interface
â”œâ”€â”€ embeddings_index/
â”‚   â””â”€â”€ faiss.index           # FAISS vector database
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ .env                      # Environment variables
â””â”€â”€ README.md                 # This file
```

## ğŸ¯ Use Cases

### Target Audience
- **ğŸ¢ EU Financial Institutions**: Banks, investment firms, insurance companies
- **ğŸ’³ Payment Service Providers**: Payment processors, e-money institutions
- **ğŸ“Š Data Service Providers**: Trade repositories, data reporting services
- **ğŸ—ï¸ ICT Service Providers**: Third-party technology vendors

### Example Queries
```
"What are the requirements for financial entities?"
"What are the ICT risk management requirements for financial entities?"
"How should we implement business continuity plans under DORA?"
"What are the reporting obligations for major ICT-related incidents?"
"What governance arrangements are required for ICT third-party risk?"
```

## ğŸ“ˆ Advanced Features

### Retrieval Analytics
- **ğŸ“Š Stage-by-Stage Visibility**: Track initial retrieval â†’ reranking â†’ final selection
- **ğŸ¯ Relevance Scoring**: Detailed reranking scores and position changes
- **ğŸ“ Document Metadata**: Source tracking and content analysis
- **â±ï¸ Performance Metrics**: Response times and processing statistics

### Legal Advisory Mode
- **âš–ï¸ Professional Tone**: Responses formatted as legal advisory guidance
- **ğŸ“– Article References**: Specific regulatory citations when available
- **ğŸ” Compliance Focus**: Actionable steps for regulatory adherence
- **âŒ Hallucination Prevention**: Strict context-only responses

## ğŸ› ï¸ Development

### Adding New Documents
1. Place PDF in project root
2. Update `CHUNKS_FILE` in `src/main.py`
3. Run chunking process: `python src/utils.py`
4. Restart application to rebuild embeddings

### Customizing the Legal Prompt
Edit `src/generator_prompt.txt` to modify the legal advisor persona and response format.

### Model Upgrades
- **Embeddings**: Update `MODEL_NAME` in `src/embeddings.py`
- **Reranking**: Update `RERANKING_MODEL` in `src/reranker.py`
- **LLM**: Add new models to frontend selection options

## ğŸ“‹ Requirements

### Core Dependencies
- `streamlit>=1.25.0` - Web interface
- `langchain-core>=0.3.29` - LLM framework
- `langchain-google-genai>=2.0.0` - Gemini integration
- `sentence-transformers>=2.2.2` - Embeddings and reranking
- `faiss-cpu>=1.7.4` - Vector database
- `pdfplumber>=0.10.1` - PDF processing
- `python-dotenv>=1.0.0` - Environment management

### Optional Dependencies
- `faiss-gpu` - GPU acceleration for vector operations
- `langchain-ollama` - Local LLM support

## ğŸ¤ Contributing

1. Fork the repository
2. Create feature branch: `git checkout -b feature/new-feature`
3. Commit changes: `git commit -m "Add new feature"`
4. Push to branch: `git push origin feature/new-feature`
5. Submit pull request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ”— Related Resources

- **[DORA Regulation](https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:32022R2554)** - Official EU regulation text
- **[Streamlit Documentation](https://docs.streamlit.io/)** - UI framework
- **[LangChain Documentation](https://python.langchain.com/)** - RAG framework
- **[Sentence Transformers](https://www.sbert.net/)** - Embedding models

---

**âš ï¸ Disclaimer**: This chatbot provides informational guidance based on DORA regulation documents. Always consult qualified legal professionals for definitive legal advice and compliance strategies.
